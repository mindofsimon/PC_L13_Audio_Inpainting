{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "PC-Image2Image_Translation_Pix2Pix.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yogdziex7Gf7"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf_HUOab7Gf8"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display\n",
        "import ntpath\n",
        "import librosa\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmARDW0m7SqD"
      },
      "source": [
        "!pip install -q -U tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TkMNqDNDodu"
      },
      "source": [
        "# let's try with MOBIPHONE dataset\n",
        "!wget --no-check-certificate -r \"https://drive.google.com/uc?export=download&id=1MflXkdaeAFyBftKkCTTuUXkfJ-fOTzus\" -O \"MOBIPHONE.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ytlscj6mD8jg"
      },
      "source": [
        "!unzip MOBIPHONE.zip\n",
        "PATH = 'MOBIPHONE/'\n",
        "folders = [\"Apple iPhone 5\", \"HTC desire c\", \"HTC Sensation xe\", \"LG GS290\", \"LG L3\", \"LG Optimus L5\", \"LG Optimus L9\", \"Nokia 5530\", \"Nokia C5\", \"Nokia N70\", \"Samsung e1230\", \"Samsung E2121B\", \"Samsung E2600\", \"Samsung Galaxy GT-I9100 s2\", \"Samsung GT-I8190 mini\", \"Samsung GT-N7100 (galaxy note2)\", \"Samsung Galaxy Nexus S\", \"Samsung s5830i\", \"Sony Ericson c902\", \"Sony ericson c510i\"]\n",
        "# \"Vodafone joy 845\" will be used for test, all of the above ones for training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIk6FoRWRmLU"
      },
      "source": [
        "#to find length of shortest file (317427)\n",
        "\n",
        "#minimum_length = 900000\n",
        "#for i in range(len(folders)):\n",
        "  #for audio in os.listdir(PATH+folders[i]+\"/\"):\n",
        "    #ext = os.path.splitext(audio)[-1].lower()\n",
        "    #if ext == \".wav\":\n",
        "      #y, sr = librosa.load(PATH+folders[i]+\"/\"+audio)\n",
        "      #if len(y) < minimum_length:\n",
        "        #minimum_length = len(y)\n",
        "\n",
        "minimum_length = 317427"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owsFDWgov4NG"
      },
      "source": [
        "# now, let's convert audio files into images...\n",
        "!mkdir frequency_images_train # directory where to save frequency representation images for train\n",
        "!mkdir frequency_images_test # directory where to save frequency representation images for test\n",
        "\n",
        "# ask input parameters\n",
        "#data = input(\"1-STFT / 2-LOG MEL SPECT\\n\")\n",
        "#reconstruction = input(\"1-Original Phase / 2-Griffinlim\\n\")\n",
        "#while (data != \"1\" and data != \"2\") or (reconstruction != \"1\" and reconstruction != \"2\"):\n",
        "#    data = input(\"1-STFT / 2-LOG MEL SPECT\\n\")\n",
        "#    reconstruction = input(\"1-Original Phase / 2-Griffinlim\\n\")\n",
        "\n",
        "\n",
        "\n",
        "#phase can be discarded for now...it will just be useful to go back to audio for the testing cases\n",
        "\n",
        "n_files = 24 # inside test folder\n",
        "i = 1 # index of output file (progressive)\n",
        "starting_point=50 # where to start mask\n",
        "m = 8 # to have a total degradation of 24 rows/columns\n",
        "data = \"2\"\n",
        "\n",
        "\n",
        "#training folders cycle\n",
        "id=1 #progressive ID, just to rename images\n",
        "for i in range(len(folders)-1):\n",
        "  for audio in os.listdir(PATH+folders[i]+\"/\"):\n",
        "    ext = os.path.splitext(audio)[-1].lower()\n",
        "    if ext == \".wav\":\n",
        "      y, sr = librosa.load(PATH+folders[i]+\"/\"+audio, sr=16000)\n",
        "      if len(y)>=20*sr-1:\n",
        "        y = y[0:20*sr]\n",
        "        if data == \"1\":\n",
        "          freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "          ph = np.angle(freq_rep)\n",
        "        else:\n",
        "          freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "          ph = np.angle(freq_rep)\n",
        "          freq_rep = librosa.feature.melspectrogram(S=abs(freq_rep), n_mels=512, sr=16000, power=1.0)\n",
        "\n",
        "        if data == \"1\":\n",
        "          mag_db = librosa.amplitude_to_db(np.abs(freq_rep))\n",
        "        else:\n",
        "          mag_db = librosa.amplitude_to_db(freq_rep)  # mel\n",
        "\n",
        "        # max and min values (in db) for conversion\n",
        "        max_val = np.max(np.max(mag_db))\n",
        "        min_val = np.min(np.min(mag_db))\n",
        "        # converting into image\n",
        "        freq_rep_img = np.round(((mag_db-min_val)/(max_val-min_val))*(255-0)+0)\n",
        "\n",
        "        #create also masked image\n",
        "        mask = np.ones([len(freq_rep_img), len(freq_rep_img[0])])\n",
        "        mask[starting_point:(starting_point+m+1), :] = 0\n",
        "\n",
        "        corrupted_img = mask * freq_rep_img\n",
        "\n",
        "        composed = np.zeros([len(freq_rep_img), len(freq_rep_img[0])*2])\n",
        "        composed[:,0:len(freq_rep_img[0])] = freq_rep_img\n",
        "        composed[:,len(freq_rep_img[0]):len(freq_rep_img[0])*2] = corrupted_img\n",
        "\n",
        "        composed_image = Image.fromarray(composed.astype(np.uint8))  \n",
        "        composed_image.save(\"frequency_images_train/speaker_train_\"+str(id)+\".png\")\n",
        "        id=id+1\n",
        "\n",
        "        #IDEA: \n",
        "        # -build one big image composed by original|corrupted\n",
        "        # -save it\n",
        "        # -decompose it in load() function...as it was done for original Pix2Pix\n",
        "\n",
        "#testing folder\n",
        "m = 8 # just to start filling it\n",
        "id=1\n",
        "for audio in sorted(os.listdir(PATH+\"Vodafone joy 845/\")):\n",
        "  ext = os.path.splitext(audio)[-1].lower()\n",
        "  if ext == \".wav\":\n",
        "      y, sr = librosa.load(PATH+\"Vodafone joy 845/\"+audio, sr = 16000)\n",
        "      y = y[0:20*sr]\n",
        "      if data == \"1\":\n",
        "         freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "         ph = np.angle(freq_rep)\n",
        "      else:\n",
        "        freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "        ph = np.angle(freq_rep)\n",
        "        freq_rep = librosa.feature.melspectrogram(S=abs(freq_rep), n_mels=512, sr=16000, power=1.0)\n",
        "\n",
        "      if data == \"1\":\n",
        "        mag_db = librosa.amplitude_to_db(np.abs(freq_rep))\n",
        "      else:\n",
        "        mag_db = librosa.amplitude_to_db(freq_rep)  # mel\n",
        "\n",
        "      # max and min values (in db) for conversion\n",
        "      max_val = np.max(np.max(mag_db))\n",
        "      min_val = np.min(np.min(mag_db))\n",
        "      # converting into image\n",
        "      freq_rep_img = np.round(((mag_db-min_val)/(max_val-min_val))*(255-0)+0)\n",
        "\n",
        "      #create also masked image\n",
        "      mask = np.ones([len(freq_rep_img), len(freq_rep_img[0])])\n",
        "      mask[starting_point:(starting_point+m+1), :] = 0\n",
        "\n",
        "      corrupted_img = mask * freq_rep_img\n",
        "\n",
        "      composed = np.zeros([len(freq_rep_img), len(freq_rep_img[0])*2])\n",
        "      composed[:,0:len(freq_rep_img[0])] = freq_rep_img\n",
        "      composed[:,len(freq_rep_img[0]):len(freq_rep_img[0])*2] = corrupted_img\n",
        "\n",
        "      composed_image = Image.fromarray(composed.astype(np.uint8))\n",
        "      save_name = audio[:-4] #to eliminate '.wav' from file name  \n",
        "      composed_image.save(\"frequency_images_test/speaker_test_\"+str(id)+\".png\")\n",
        "      id=id+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJToabWq-yqf"
      },
      "source": [
        "# Params\n",
        "BUFFER_SIZE = 400\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 512\n",
        "IMG_HEIGHT = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAWuUiaF-2ZP"
      },
      "source": [
        "# Load real and input(corrupted) image\n",
        "\n",
        "def load(image_file):\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.image.decode_png(image)\n",
        "  w = tf.shape(image)[1]\n",
        "\n",
        "  w = w // 2\n",
        "  real_image = image[:, :w, :]\n",
        "  input_image = image[:, w:, :]\n",
        "\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt6pfTfvAX3v"
      },
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOWfrnQ-ACAG"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obl4V2ZNAKeF"
      },
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T9MV0hqALsa"
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIq-7GTCARr1"
      },
      "source": [
        "@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "  # resizing to 286 x 286 x 3\n",
        "  input_image, real_image = resize(input_image, real_image, IMG_HEIGHT, IMG_WIDTH)\n",
        "\n",
        "  # randomly cropping to 256 x 256 x 3\n",
        "  #input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "  #if tf.random.uniform(()) > 0.5:\n",
        "    # random mirroring\n",
        "    #input_image = tf.image.flip_left_right(input_image)\n",
        "    #real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2UiVd7KAn30"
      },
      "source": [
        "Random jittering does the following:\n",
        "* Resize an image to a bigger height and width\n",
        "* Randomly crop to target size\n",
        "* Randomly flip the image horizontally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLQ9_UJ7A04m"
      },
      "source": [
        "def load_image_train(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  #input_image, real_image = random_jitter(input_image, real_image)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HjDpJhNA4rK"
      },
      "source": [
        "def load_image_test(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  #input_image, real_image = resize(input_image, real_image, IMG_HEIGHT, IMG_WIDTH)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piHz515xA6uo"
      },
      "source": [
        "# Input Pipeline\n",
        "\n",
        "In this example, we will use the [`tf.data`](https://www.tensorflow.org/guide/data) API, while it is not necessary to understand everything for the sake of this tutorial, check the link if you want to know more about how it works.\n",
        "\n",
        "In a nutshell it is an API which allows you to handle data and feed them into tensorflow models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWP7kgS-A9A6"
      },
      "source": [
        "train_dataset = tf.data.Dataset.list_files('frequency_images_train/*.png')\n",
        "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yir10d56A_PY"
      },
      "source": [
        "test_dataset = tf.data.Dataset.list_files('frequency_images_test/*.png', shuffle=False)\n",
        "test_dataset = test_dataset.map(load_image_test,deterministic=True)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soimSsmXo7rq"
      },
      "source": [
        "#import keras.backend as K\n",
        "#test_dataset = tf.data.Dataset.list_files('frequency_images_test/*.png', shuffle=False)\n",
        "#for f in test_dataset.take(4):\n",
        "#  print(K.eval(f))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Mu1gZIBFN2"
      },
      "source": [
        "# Build the generator\n",
        "  * Architecture is a modified [U-Net](https://arxiv.org/abs/1505.04597)\n",
        "  * Each block in the encoder is (Conv -> [Batchnorm](https://arxiv.org/abs/1502.03167) -> [Leaky ReLU](https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf))\n",
        "  * There are [skip connections](https://en.wikipedia.org/wiki/Residual_neural_network) between the encoder and decoder (as in U-Net).\n",
        "\n",
        "  ![unet_architecture](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Architecture-of-the-U-Net-Generator-Model.png)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfmH1F-3BH93"
      },
      "source": [
        "OUTPUT_CHANNELS = 1 # gray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_CKHc0By-u"
      },
      "source": [
        "## Downsampling layer: FILL THE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvisNp4wBh3y"
      },
      "source": [
        "#FILL THE CODE: complete the downsampling layer that we will use to shrink the image input\n",
        "\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "#FILL THE CODE: Add a 2D convolutional layer, with the specified options, use strides=2, padding=\"same\" and use_bias=False\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGInXDbsB3Yu"
      },
      "source": [
        "## Upsampling Layer: FILL THE CODE\n",
        "\n",
        "To upsample the images we are going to use a [`Conv2DTranspose`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layer, which performs a [transposed convolution](https://towardsdatascience.com/what-is-transposed-convolutional-layer-40e5e6e31c11#:~:text=Transposed%20convolutions%20are%20standard%20convolutions,in%20a%20standard%20convolution%20operation.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZkXG5y8B5fV"
      },
      "source": [
        "#FILL THE CODE: complete the upsampling layer that we will increase image size after downsampling it\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "   #FILL THE CODE: Add a 2D transposed convolutional layer, with the specified options, use strides=2, padding='same' and use_bias=False \n",
        "\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYqkvD-vCJTH"
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[512, 512, 1])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(32, 4, apply_batchnorm=False),\n",
        "    downsample(64, 4), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "    upsample(32, 4)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh') # (bs, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ewAk_8CNcq"
      },
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iBl8eQrDh45"
      },
      "source": [
        "   ## Generator Loss: FILL THE CODE\n",
        "    * sigmoid cross entropy loss of generated images and and array of ones (classification)\n",
        "    * L1 loss -> Mean Absolute Error (MAE) between generated image and target image\n",
        "      * this allows generated image to be structurally similar to target image\n",
        "      * Formula to calculate total generator loss is:\n",
        "        * $ \\mathcal{L}_\\text{gan} +\\lambda \\mathcal{L}_\\text{L1} $ \n",
        "        \n",
        "        with $\\lambda = 100$ (decided by authors of the original paper)\n",
        "     \n",
        "\n",
        "```\n",
        "# Questo Ã¨ formattato come codice\n",
        "```\n",
        "\n",
        "\n",
        "The training procedure for the generator is shown below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj3fMvhqEINA"
      },
      "source": [
        "LAMBDA = 100\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qARKmxRGzgr"
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jvp4M_VERHQ"
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  # FILL THE CODE:\n",
        "  # define the GAN generator loss i.e. does the discriminator classify generator \n",
        "  # outputs as real?\n",
        "  # HINT: use loss_object to compute the loss and tf.ones_like() to represent classification\n",
        "  # result\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "\n",
        "  # FILL THE CODE: mean absolute error\n",
        "  # compute mean absolute error between target image and generated image\n",
        "  # HINT: use tf.reduce_mean and tf.abs\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  # FILL THE CODE: compute generator loss as the sum of GAN loss and weighted\n",
        "  # L1 loss\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttGX7STO35p8"
      },
      "source": [
        "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWVQXVNMEbTg"
      },
      "source": [
        "# Build the Discriminator\n",
        "Discriminator is a PatchGAN\n",
        "  * Each block is (Conv -> BatchNorm -> LeakyRelu)\n",
        "  * shape of output after last layer is (bs,30,30,1)\n",
        "    * N.B.-> each 30x30 patch of the output classifies a 70x70 portion of the input (PatchGAN)\n",
        "  * Discriminator receives two inputs.\n",
        "    * Input_1 image and target image, which it should classify as **real**.\n",
        "    * Input_2 image and generated image, which it should classify as **fake**.\n",
        "    * We concatenate Input_1 and Input_2 together in the code (```# tf.concat([inp, tar], axis=-1)```)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADNN7OPNEdNk"
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[512, 512, 1], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[512, 512, 1], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "  down4 = downsample(512, 4)(down3) # (bs, 16, 16, 512)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down4) # (bs, 34, 34, 256) ORIGINALLY WAS ON DOWN3!\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12L6FMgtFkxZ"
      },
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLu5lxTlFx4J"
      },
      "source": [
        "## Discriminator Loss: FILL THE CODE\n",
        "The discriminator loss function takes 2 inputs:\n",
        "* real images\n",
        "  * real_loss is a sigmoid cross entropy loss of the real images and an array of ones(since these are the real images)\n",
        "* generated images\n",
        "  * generated_loss is a sigmoid cross entropy loss of the generated images and an array of zeros(since these are the fake images)\n",
        "* Total_loss is a sum of real_loss and generated_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q1QHLMrG96E"
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  # FILL THE CODE:\n",
        "  # define the GAN discriminator loss remember ideally discriminator should classify\n",
        "  # as 0 fake images and as 1 real images\n",
        "  # HINT: use loss_object and tf.ones_like and tf.zeros_like\n",
        "\n",
        "  # loss related to real images\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  # loss related to fake images\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # FILL THE CODE:\n",
        "  # compute output loss as the sum of real and generated loss\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq78AkQtHJyg"
      },
      "source": [
        "# Define the Optimizers and Checkpoint-saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu3iHB0nHAff"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DdvnIlDHC9y"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuCEvKrMHMlv"
      },
      "source": [
        "# Generate Images\n",
        "\n",
        "Write a function to plot some images during training:\n",
        "  * We pass images from the test dataset to the generator\n",
        "  * The generator will then translate the input image into the output\n",
        "  * Last step is to plot the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGqVax0YHD_6"
      },
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    img_to_print = display_list[i].numpy()\n",
        "    img_to_print = img_to_print.squeeze() # to adapt it for imshow\n",
        "    plt.imshow(img_to_print)\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  return display_list[2].numpy(), display_list[1].numpy() #returning the predicted image and the original"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnbWBXPXHhBg"
      },
      "source": [
        "for example_input, example_target in test_dataset.take(1):\n",
        "  generate_images(generator, example_input, example_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbWbpuj3H5mn"
      },
      "source": [
        "# Training: FILL THE CODE\n",
        "* For each example input generate an output\n",
        "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
        "* Calculate Generator_loss and discriminator_loss\n",
        "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
        "* Then log the losses to TensorBoard.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx4f1-V7IKet"
      },
      "source": [
        "EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX4AYj8uILYw"
      },
      "source": [
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XQMnvyXIOt0"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, epoch):\n",
        "\n",
        "  # Use tf.gradient tape to keep track of the generator and discriminator gradients\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "    # FILL THE code: compute the output of the generator (i.e. fake images)   \n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    # FILL THE CODE: compute the output of the discriminator w.r.t. real images \n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    # FILL THE CODE: compute the output of the discriminator w.r.t. fake images \n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    # compute the 2 losses\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  # FILL THE CODE: \n",
        "  # Compute the gradient of the generator loss w.r.t. the generator parameters \n",
        "  # HINT: gen_tape.gradient(...), disc_tape.gradient(...)   \n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "    # Compute the gradient of the discriminator loss w.r.t. the discriminator parameters\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  # Apply the gradient to the optimizer so that it can update the model accordingly  \n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKwszdZ7IQYc"
      },
      "source": [
        "Actual training loop:\n",
        "\n",
        "* Iterates over the number of epochs.\n",
        "\n",
        "* On each epoch it clears the display, and runs generate_images to show it's progress.\n",
        "* On each epoch it iterates over the training dataset, printing a '.' for each example.\n",
        "* It saves a checkpoint every 20 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeMqVkOtIZLT"
      },
      "source": [
        "def fit(train_ds, epochs, test_ds):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    for example_input, example_target in test_ds.take(1):\n",
        "      generate_images(generator, example_input, example_target)\n",
        "    print(\"Epoch: \", epoch)\n",
        "\n",
        "    # Train\n",
        "    for n, (input_image, target) in train_ds.enumerate():\n",
        "      print('.', end='')\n",
        "      if (n+1) % 100 == 0:\n",
        "        print()\n",
        "      train_step(input_image, target, epoch)\n",
        "    print()\n",
        "\n",
        "    # saving (checkpoint) the model every 20 epochs\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                        time.time()-start))\n",
        "  checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjiT_irZIeHI"
      },
      "source": [
        "This training loop saves logs you can easily view in TensorBoard to monitor the training progress. Working locally you would launch a separate tensorboard process. In a notebook, if you want to monitor with TensorBoard it's easiest to launch the viewer before starting the training.\n",
        "\n",
        "To launch the viewer paste the following into a code-cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovnahmdvIeqy"
      },
      "source": [
        "% load_ext tensorboard\n",
        "% tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XXHQxv3I9VZ"
      },
      "source": [
        "Run the training loop:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcyZldPzI-5w"
      },
      "source": [
        "fit(train_dataset, EPOCHS, test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Oqx9vsJzwn"
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
        "    width=\"100%\",\n",
        "    height=\"1000px\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z1dtd8pKqIJ"
      },
      "source": [
        "Interpreting the logs from a GAN is more subtle than a simple classification or regression model. Things to look for::\n",
        "\n",
        "* Check that neither model has \"won\". If either the gen_gan_loss or the disc_loss gets very low it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
        "\n",
        "* The value log(2) = 0.69 is a good reference point for these losses, as it indicates a perplexity of 2: That the discriminator is on average equally uncertain about the two options.\n",
        "\n",
        "* For the disc_loss a value below 0.69 means the discriminator is doing better than random, on the combined set of real+generated images.\n",
        "\n",
        "*For the gen_gan_loss a value below 0.69 means the generator i doing better than random at foolding the descriminator.\n",
        "\n",
        "*As training progresses the gen_l1_loss should go down"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVnb4z4XK733"
      },
      "source": [
        "# Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfXhMNBeLAG2"
      },
      "source": [
        "!ls {checkpoint_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe0WpJmuLThI"
      },
      "source": [
        "# Run the trained model on a few examples from the test dataset\n",
        "for inp, tar in test_dataset.take(5):\n",
        "  out = generate_images(generator, inp, tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTaDb1I7VsqK"
      },
      "source": [
        "# Take generated images and go back to audio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbQuu216Vudv"
      },
      "source": [
        "def convert(freq_img, ph, data, reconstruction, max_val, min_val, sr):\n",
        "  inpainted_freq_rep_db = ((freq_img-0)/(255-0))*(max_val-min_val)+min_val\n",
        "  inpainted_freq_rep = librosa.db_to_amplitude(inpainted_freq_rep_db)\n",
        "\n",
        "  if data == \"1\": #stft\n",
        "    if reconstruction == \"1\": #original phase\n",
        "      #inpainted_freq_rep = librosa.db_to_amplitude(inpainted_freq_rep_db)\n",
        "      real = inpainted_freq_rep * np.cos(ph)\n",
        "      imag = inpainted_freq_rep * np.sin(ph)\n",
        "      mod_freq_rep = np.zeros([len(real), len(real[0])], dtype=np.complex_)\n",
        "      # inverse transform (creating a complex stft from modified magnitude and original phase)\n",
        "      for i in range(len(real)):\n",
        "        for j in range(len(real[0])):\n",
        "          mod_freq_rep[i, j] = complex(real[i, j], imag[i, j])\n",
        "          \n",
        "      output = librosa.istft(mod_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "\n",
        "    else: #griffinlim\n",
        "      #inpainted_freq_rep = librosa.db_to_amplitude(inpainted_freq_rep_db)\n",
        "      output = librosa.griffinlim(inpainted_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "\n",
        "  else: #log mel\n",
        "    if reconstruction == \"1\": #original phase\n",
        "      #inpainted_freq_rep = librosa.db_to_power(inpainted_freq_rep_db)\n",
        "      inpainted_freq_rep = librosa.feature.inverse.mel_to_stft(inpainted_freq_rep, sr, n_fft=1023, power=1.0)\n",
        "      real = inpainted_freq_rep * np.cos(ph)\n",
        "      imag = inpainted_freq_rep * np.sin(ph)\n",
        "      mod_freq_rep = np.zeros([len(real), len(real[0])], dtype=np.complex_)\n",
        "      # inverse transform (creating a complex stft from modified magnitude and original phase)\n",
        "      for i in range(len(real)):\n",
        "        for j in range(len(real[0])):\n",
        "          mod_freq_rep[i, j] = complex(real[i, j], imag[i, j])\n",
        "      \n",
        "      output = librosa.istft(mod_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "      \n",
        "    else: #griffinlim\n",
        "      #inpainted_freq_rep = librosa.db_to_power(inpainted_freq_rep_db)\n",
        "      mod_freq_rep = librosa.feature.inverse.mel_to_stft(inpainted_freq_rep, sr, n_fft=1023,power=1.0)\n",
        "      output = librosa.griffinlim(mod_freq_rep, win_length=625, hop_length=625, window='rect')\n",
        "    \n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzGRfl0eWMMI"
      },
      "source": [
        "from math import log10, sqrt\n",
        "\n",
        "def wada_snr(wav):\n",
        "    # Direct blind estimation of the SNR of a speech signal.\n",
        "    #\n",
        "    # Paper on WADA SNR:\n",
        "    #   http://www.cs.cmu.edu/~robust/Papers/KimSternIS08.pdf\n",
        "    #\n",
        "    # This function was adapted from this matlab code:\n",
        "    #   https://labrosa.ee.columbia.edu/projects/snreval/#9\n",
        "    #\n",
        "    # MIT license, John Meade, 2020\n",
        "\n",
        "    # init\n",
        "    eps = 1e-10\n",
        "    # next 2 lines define a fancy curve derived from a gamma distribution -- see paper\n",
        "    db_vals = np.arange(-20, 101)\n",
        "    g_vals = np.array([0.40974774, 0.40986926, 0.40998566, 0.40969089, 0.40986186, 0.40999006, 0.41027138, 0.41052627, 0.41101024, 0.41143264, 0.41231718, 0.41337272, 0.41526426, 0.4178192 , 0.42077252, 0.42452799, 0.42918886, 0.43510373, 0.44234195, 0.45161485, 0.46221153, 0.47491647, 0.48883809, 0.50509236, 0.52353709, 0.54372088, 0.56532427, 0.58847532, 0.61346212, 0.63954496, 0.66750818, 0.69583724, 0.72454762, 0.75414799, 0.78323148, 0.81240985, 0.84219775, 0.87166406, 0.90030504, 0.92880418, 0.95655449, 0.9835349 , 1.01047155, 1.0362095 , 1.06136425, 1.08579312, 1.1094819 , 1.13277995, 1.15472826, 1.17627308, 1.19703503, 1.21671694, 1.23535898, 1.25364313, 1.27103891, 1.28718029, 1.30302865, 1.31839527, 1.33294817, 1.34700935, 1.3605727 , 1.37345513, 1.38577122, 1.39733504, 1.40856397, 1.41959619, 1.42983624, 1.43958467, 1.44902176, 1.45804831, 1.46669568, 1.47486938, 1.48269965, 1.49034339, 1.49748214, 1.50435106, 1.51076426, 1.51698915, 1.5229097 , 1.528578  , 1.53389835, 1.5391211 , 1.5439065 , 1.54858517, 1.55310776, 1.55744391, 1.56164927, 1.56566348, 1.56938671, 1.57307767, 1.57654764, 1.57980083, 1.58304129, 1.58602496, 1.58880681, 1.59162477, 1.5941969 , 1.59693155, 1.599446  , 1.60185011, 1.60408668, 1.60627134, 1.60826199, 1.61004547, 1.61192472, 1.61369656, 1.61534074, 1.61688905, 1.61838916, 1.61985374, 1.62135878, 1.62268119, 1.62390423, 1.62513143, 1.62632463, 1.6274027 , 1.62842767, 1.62945532, 1.6303307 , 1.63128026, 1.63204102])\n",
        "\n",
        "    # peak normalize, get magnitude, clip lower bound\n",
        "    wav = np.array(wav)\n",
        "    wav = wav / np.max(np.abs(wav))\n",
        "    abs_wav = np.abs(wav)\n",
        "    abs_wav[abs_wav < eps] = eps\n",
        "\n",
        "    # calcuate statistics\n",
        "    # E[|z|]\n",
        "    v1 = max(eps, abs_wav.mean())\n",
        "    # E[log|z|]\n",
        "    v2 = np.log(abs_wav).mean()\n",
        "    # log(E[|z|]) - E[log(|z|)]\n",
        "    v3 = np.log(v1) - v2\n",
        "\n",
        "    # table interpolation\n",
        "    wav_snr_idx = None\n",
        "    if any(g_vals < v3):\n",
        "        wav_snr_idx = np.where(g_vals < v3)[0].max()\n",
        "    # handle edge cases or interpolate\n",
        "    if wav_snr_idx is None:\n",
        "        wav_snr = db_vals[0]\n",
        "    elif wav_snr_idx == len(db_vals) - 1:\n",
        "        wav_snr = db_vals[-1]\n",
        "    else:\n",
        "        wav_snr = db_vals[wav_snr_idx] + \\\n",
        "            (v3-g_vals[wav_snr_idx]) / (g_vals[wav_snr_idx+1] - \\\n",
        "            g_vals[wav_snr_idx]) * (db_vals[wav_snr_idx+1] - db_vals[wav_snr_idx])\n",
        "\n",
        "    # Calculate SNR\n",
        "    dEng = sum(wav**2)\n",
        "    dFactor = 10**(wav_snr / 10)\n",
        "    dNoiseEng = dEng / (1 + dFactor) # Noise energy\n",
        "    dSigEng = dEng * dFactor / (1 + dFactor) # Signal energy\n",
        "    snr = 10 * np.log10(dSigEng / dNoiseEng)\n",
        "\n",
        "    return snr\n",
        "\n",
        "def PSNR(original, compressed):\n",
        "  mse = np.mean((original - compressed) ** 2)\n",
        "  if mse == 0:  # MSE is zero means no noise is present in the signal .\n",
        "  # Therefore PSNR have no importance.\n",
        "    return 100\n",
        "  max_pixel = 255.0\n",
        "  psnr = 20 * log10(max_pixel / sqrt(mse))\n",
        "  return psnr\n",
        "\n",
        "def alternative_SNR(y):\n",
        "\n",
        "  def energy(x):\n",
        "    e = np.sum(x ** 2)\n",
        "    return e\n",
        "\n",
        "  Z = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "  z = librosa.istft(Z, win_length=625, hop_length=625, window='rect')\n",
        "  n = y[:z.size] - z\n",
        "  snr = 10 * np.log10(energy(y)/energy(n))\n",
        "  return snr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNRDjsoOYGrx"
      },
      "source": [
        "!pip install pesq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhTYD1dyWSP2"
      },
      "source": [
        "import cv2\n",
        "from scipy.io import wavfile\n",
        "import csv\n",
        "from google.colab import files\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from pesq import pesq\n",
        "\n",
        "reconstruction = \"2\"\n",
        "\n",
        "iteration = round(m/2)+1\n",
        "n_files = 24\n",
        "file_list = sorted(os.listdir('MOBIPHONE/Vodafone joy 845/'))\n",
        "image_list = sorted(os.listdir('frequency_images_test/'))\n",
        "input_indices = [\"1\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"2\",\"20\",\"21\",\"21\",\"22\",\"23\",\"24\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
        "#THE CORRESPONDANCE IS (ex) : speaker_10.png --> speaker18.wav --> output_2.wav and so on...\n",
        "\n",
        "i = 0\n",
        "# delete old csv if present\n",
        "if os.path.isfile('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv'):\n",
        "  os.remove('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv')\n",
        "\n",
        "# creating csv file where to save results (SNR/PSNR)\n",
        "with open('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv', mode='w') as pix_2_pix: # OPENING FILE (TILL THE END)\n",
        "  pix_2_pix_writer = csv.writer(pix_2_pix, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  pix_2_pix_writer.writerow(['FILE', 'SNR', 'PSNR', 'SSIM', 'PESQ'])\n",
        "  #so, for example...model_based_results_1_1.csv means results of data = 1 (stft) and reconstruction = 1 (original phase)\n",
        "  for inp, tar in test_dataset.take(n_files):\n",
        "    print(\"\\nFILE \"+str(i+1)+\"...\")\n",
        "    out, orig = generate_images(generator, inp, tar)\n",
        "        \n",
        "    #recovering phase\n",
        "    index = int(input_indices[i])\n",
        "    audio_to_load = file_list[index-1]\n",
        "    y, sr = librosa.load('MOBIPHONE/Vodafone joy 845/'+audio_to_load, sr = 16000)\n",
        "    y = y[0:20*sr]\n",
        "    if data == \"1\":\n",
        "      freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "      ph = np.angle(freq_rep)\n",
        "    else:\n",
        "      freq_rep = librosa.stft(y, n_fft=1023, win_length=625, hop_length=625, window='rect')\n",
        "      ph = np.angle(freq_rep)\n",
        "      freq_rep = librosa.feature.melspectrogram(S=abs(freq_rep), n_mels=512, sr=16000, power=1.0)\n",
        "\n",
        "    # recovering also original image (for PSNR)\n",
        "    if data == \"1\":\n",
        "      mag_db = librosa.amplitude_to_db(np.abs(freq_rep))\n",
        "    else:\n",
        "      mag_db = librosa.amplitude_to_db(freq_rep)  # mel\n",
        "\n",
        "    # max and min values (in db) for conversion\n",
        "    max_val = np.max(np.max(mag_db))\n",
        "    min_val = np.min(np.min(mag_db))\n",
        "    # converting into image\n",
        "    img_np = np.round(((mag_db-min_val)/(max_val-min_val))*(255-0)+0)\n",
        "\n",
        "    out_np = out #will be the last inpainted (predicted) image\n",
        "    orig_np = orig #will be the original image\n",
        "\n",
        "    # converting into 0,255\n",
        "    max_val_orig = np.max(np.max(orig_np[:,:,0]))\n",
        "    min_val_orig = np.min(np.min(orig_np[:,:,0]))\n",
        "    max_o = np.max(np.max(out_np[:,:,0]))\n",
        "    min_o = np.min(np.min(out_np[:,:,0]))\n",
        "    # converting into image\n",
        "    orig_np = np.round(((orig_np[:,:,0]-min_val_orig)/(max_val_orig-min_val_orig))*(255-0)+0)\n",
        "    out_np = np.round(((out_np[:,:,0] - min_o) / (max_o - min_o)) * (255 - 0) + 0)\n",
        "      \n",
        "    #PSNR\n",
        "    psnr = PSNR(orig_np[:,:], out_np[:,:])\n",
        "        \n",
        "    #resizing to original size\n",
        "    #out_np = cv2.resize(out_np, dsize=(len(freq_rep_img[0]), len(freq_rep_img)), interpolation=cv2.INTER_CUBIC)\n",
        "        \n",
        "    min_val_out = np.min(np.min(out_np[:,:]))\n",
        "    max_val_out = np.max(np.max(out_np[:,:]))\n",
        "\n",
        "    # here I need to calculate the original phase\n",
        "    output = convert(out_np[:,:], ph, data, reconstruction, max_val, min_val, sr)\n",
        "    min_out = np.min(output)\n",
        "    max_out = np.max(output)\n",
        "    output=((output-min_out)/(max_out-min_out))*(max(y)-min(y))+min(y) # converting back\n",
        "        \n",
        "    # writing output\n",
        "    wavfile.write('output_'+str(i+1)+'.wav', sr, output.astype(np.float32))\n",
        "\n",
        "    # SNR\n",
        "    o, sr = librosa.load('output_'+str(i+1)+'.wav',sr=16000)\n",
        "    snr = alternative_SNR(o)\n",
        "          \n",
        "    #PESQ\n",
        "    psq = pesq(16000, y, o, 'wb')\n",
        "\n",
        "    # SSIM\n",
        "    images_ssim = ssim(orig_np[:,:], out_np[:,:], data_range=orig_np[:,:].max() - out_np[:,:].min())\n",
        "\n",
        "    # save results in csv\n",
        "    pix_2_pix_writer.writerow([os.path.basename(str('MOBIPHONE/Vodafone joy 845/'+str(audio_to_load[:-4]))), str(snr), str(psnr), str(images_ssim), str(psq)])\n",
        "\n",
        "    i = i + 1\n",
        "\n",
        "\n",
        "# download csv\n",
        "files.download('pix_2_pix_results_'+str(data)+'_'+str(reconstruction)+'_'+str(iteration)+'_.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICz6fehZw_Jd"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n"
      ]
    }
  ]
}